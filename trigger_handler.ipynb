{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7744d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce\n",
    "import re\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b64de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject(subject_code='A001',path='LOG/'):\n",
    "    \"\"\"\n",
    "    Read path, then search in given path person with\n",
    "    given subject code'\n",
    "    \"\"\"\n",
    "    files = [file for file in os.listdir('LOG/') if 'log' in file]\n",
    "    return [file for file in files if subject_code in file]\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Read file, check whether it contains proper\n",
    "    columns (based on first one) and return \n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    #read file, skiping rows with metadata\n",
    "    file = pd.read_csv(filename,sep=\"\\t\",skiprows=2)\n",
    "    #check columns (whether first one is subject)\n",
    "    if np.any(file.columns.str.contains('Subject')):\n",
    "        #delete Response (irrelevant for this study, but \n",
    "        #ther are some troubles with it)\n",
    "        file = file.loc[file['Event Type'] != 'Response']\n",
    "        #if columns are proper, restart index and return\n",
    "        return file.reset_index()\n",
    "    else:\n",
    "        #if columns are not proper, read with skipping\n",
    "        #different n of rows\n",
    "        file = pd.read_csv(filename,\"\\t\",skiprows=3)\n",
    "        #drop unnecessary rows\n",
    "        file = file.dropna(subset = ['Subject'])\n",
    "        #delete Response (irrelevant for this study, but \n",
    "        #ther are some troubles with it)\n",
    "        file = file.loc[file['Event Type'] != 'Response']\n",
    "        #if columns are proper, restart index and return\n",
    "        return file.reset_index()\n",
    "    \n",
    "def get_arg_block(data,blockname='block'):\n",
    "    \"\"\"Get arg of blocks in data\"\"\"\n",
    "    return (data\n",
    "            .loc[(data['Code']\n",
    "                  .str.lower()\n",
    "                  .str.contains(blockname.lower())),:]\n",
    "            .index)\n",
    "\n",
    "def time_correction(data):\n",
    "    \"\"\"\n",
    "    Corrects time in data from Presentation. \n",
    "    Reduce all Time column by time value of first event, \n",
    "    then divides by 10000 (to turn into ms).\n",
    "    Returns whole DataFrame.\n",
    "    \"\"\"\n",
    "    data['Time'] = (data['Time'] - data['Time'].iloc[0])/10000\n",
    "    return data\n",
    "\n",
    "def create_file(filename,fixed_window=0):\n",
    "    \"\"\"\n",
    "    Creates file with given filename\n",
    "    \"\"\"\n",
    "    df = read_file(filename)\n",
    "    df = time_correction(df)\n",
    "    args = get_arg_block(df,r'S*P')\n",
    "    df = df.loc[args]\n",
    "\n",
    "    result = pd.DataFrame(columns=['Code','Onset','Offset'])\n",
    "    for ind in range(len(df)):\n",
    "        part = df.iloc[ind][['Code','Time']]\n",
    "        try:\n",
    "            if 'inter' in part['Code']:\n",
    "                offset = part['Time'] + 15\n",
    "                duration = 15\n",
    "            else:\n",
    "                offset = df.iloc[ind+1]['Time']\n",
    "                duration = 13\n",
    "            if fixed_duration>0:\n",
    "                duration = fixed_duration\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        result = pd.concat([result,\n",
    "                            pd.DataFrame({'Code':part['Code'],\n",
    "                             'Onset':part['Time'],\n",
    "                             'Offset':offset,\n",
    "                             'Duration':duration},index=[0])])\n",
    "    return result.reset_index()\n",
    "\n",
    "\n",
    "def create_file_end_recording(filename):\n",
    "    \"\"\"\n",
    "    Creates file with given filename; \n",
    "    it is alternative way, in which \n",
    "    \"\"\"\n",
    "    df = read_file(filename)\n",
    "    df = time_correction(df)\n",
    "    args = get_arg_block(df,r'S*P')\n",
    "    df = df.loc[args]\n",
    "\n",
    "    result = pd.DataFrame(columns=['Code','Onset','Offset'])\n",
    "    for ind in range(len(df)):\n",
    "        part = df.iloc[ind][['Code','Time']]\n",
    "        try:\n",
    "            if 'end' in part['Code']:\n",
    "                offset = part['Time'] + 15\n",
    "                duration = 15\n",
    "            else:\n",
    "                offset = df.iloc[ind+1]['Time']\n",
    "                duration = 13\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        result = pd.concat([result,pd.DataFrame({'Code':part['Code'],\n",
    "                      'Onset':part['Time'],\n",
    "                      'Offset':offset,\n",
    "                      'Duration':duration},index=[0])])\n",
    "    return result.reset_index()\n",
    "\n",
    "def generate_timing_conditions(data,prefix):\n",
    "    for condition,name in zip(['S','SN'],\n",
    "                              ['krytyka','neutralne']):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4']:\n",
    "                (data.loc[data['Code']\n",
    "                      .str\n",
    "                      .contains(fr'{condition}\\d_P[{part}]{modality_sign}'),]\n",
    "                 .to_csv(f'{prefix}_{name}_{modality}_P{part}.tsv'))\n",
    "    if any(data['Code'].str.contains(fr'S\\d_P\\d_T')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4','5','6','7','4567']:\n",
    "                (data.loc[data['Code']\n",
    "                      .str\n",
    "                      .contains(fr'S\\d_P[{part}]_T{modality_sign}'),]\n",
    "                 .to_csv(f'{prefix}_therapy_{modality}_P{part}_therapy.tsv'))\n",
    "    elif any(data['Code'].str.contains(fr'SN\\d_P\\d_K')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4','5','6','7','4567']:\n",
    "                (data.loc[data['Code']\n",
    "                      .str\n",
    "                      .contains(fr'SN\\d_P[{part}]_K{modality_sign}'),]\n",
    "                 .to_csv(f'{prefix}_therapy_{modality}_P{part}.tsv'))\n",
    "                \n",
    "def FSL_fix(data):\n",
    "    \"\"\"\n",
    "    Return data, with columns necessary for analysis in FSL.\n",
    "    \"\"\"\n",
    "    data['weight'] = np.ones(len(data))\n",
    "    data.columns = ['index','trial_type','onset','offset','duration','weight']\n",
    "    data['onset'] = np.round(data['onset'])\n",
    "    return data.loc[:,['onset','duration','weight','trial_type']]\n",
    "\n",
    "def generate_timing_conditions_FSL(data,prefix):\n",
    "    data = FSL_fix(data)\n",
    "    for condition,name in zip(['S','SN'],\n",
    "                              ['krytyka','neutralne']):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4']:\n",
    "                (data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'{condition}\\d_P[{part}]{modality_sign}'),['onset','duration','weight']]\n",
    "                 .to_csv(f'{prefix}_{name}_{modality}_P{part}.tsv',\n",
    "                         '\\t',\n",
    "                         index=False,\n",
    "                         header=False))\n",
    "    if any(data['trial_type'].str.contains(fr'S\\d_P\\d_T')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4','5','6','7','4567']:\n",
    "                (data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'S\\d_P[{part}]_T{modality_sign}'),['onset','duration','weight']]\n",
    "                 .to_csv(f'{prefix}_therapy_{modality}_P{part}_therapy.tsv',\n",
    "                         '\\t',\n",
    "                         index=False,\n",
    "                         header=False))\n",
    "    elif any(data['trial_type'].str.contains(fr'SN\\d_P\\d_K')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4','5','6','7','4567']:\n",
    "                (data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'SN\\d_P[{part}]_K{modality_sign}'),['onset','duration','weight']]\n",
    "                 .to_csv(f'{prefix}_therapy_{modality}_P{part}.tsv',\n",
    "                         '\\t',\n",
    "                         index=False,\n",
    "                         header=False))\n",
    "                \n",
    "\n",
    "def generate_timing_conditions_SPM(data,prefix):\n",
    "    data = FSL_fix(data)\n",
    "    result = {'names':[],\n",
    "              'onsets':[],\n",
    "              'durations':[]}\n",
    "    for condition,name in zip(['S','SN'],\n",
    "                              ['krytyka','neutralne']):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','4']:\n",
    "                result['names'].append(fr\"{name}_{modality}_P{part}\")\n",
    "                result['onsets'].append((data.loc[data['trial_type']\n",
    "                                                  .str\n",
    "                                                  .contains(fr'{condition}\\d_P[{part}]{modality_sign}'),\n",
    "                                                  ['onset']])\n",
    "                                        .values)\n",
    "                result['durations'].append((data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'{condition}\\d_P[{part}]{modality_sign}'),['duration']]).values)\n",
    "\n",
    "    if any(data['trial_type'].str.contains(fr'S\\d_P\\d_T')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','123','4','5','6','7']:\n",
    "                result['names'].append(f'therapy_{modality}_P{part}')\n",
    "                result['onsets'].append((data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'S\\d_P[{part}]_T{modality_sign}'),['onset']]).values)\n",
    "                result['durations'].append((data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'S\\d_P[{part}]_T{modality_sign}'),['duration']]).values)\n",
    "\n",
    "    elif any(data['trial_type'].str.contains(fr'SN\\d_P\\d_K')):\n",
    "        for modality_sign,modality in zip(['$','inter'],\n",
    "                                          ['s','i']):\n",
    "            for part in ['1','2','3','4','5','6','7']:\n",
    "                result['names'].append(f'therapy_{modality}_P{part}')\n",
    "                result['onsets'].append((data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'SN\\d_P[{part}]_K{modality_sign}'),['onset']]).values)\n",
    "                result['durations'].append((data.loc[data['trial_type']\n",
    "                      .str\n",
    "                      .contains(fr'SN\\d_P[{part}]_K{modality_sign}'),['duration']]).values)\n",
    "    tmp=np.zeros(len(result['names']),\n",
    "                 dtype=object)\n",
    "    tmp[:]=result['names']\n",
    "    result['names']  = tmp\n",
    "    savemat(f'{prefix}.mat',uno)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d3c89b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = read_file('LOG/JM2022_B004_TP1-Run2_functionalScan.log')\n",
    "file = create_file('LOG/JM2022_B004_TP1-Run2_functionalScan.log')\n",
    "generate_timing_conditions_SPM(file,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4ec0997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path='LOG/'\n",
    "for raw_file in os.listdir('LOG/'):\n",
    "    #check whether file contains subject's code\n",
    "    if len(re.findall(r'b\\d+',raw_file.lower()))>0:\n",
    "        #if it contains code, try create directory with subject code,\n",
    "        #if this directory exists, pass this action.\n",
    "        subject_code = re.findall(r'b\\d+',raw_file.lower())[0].upper()\n",
    "        try:\n",
    "            os.mkdir(subject_code)\n",
    "            os.mkdir(f'{subject_code}\\ses-1')\n",
    "            os.mkdir(f'{subject_code}\\ses-2')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        #\n",
    "        files = find_subject(subject_code,path)\n",
    "        for filename in files:\n",
    "            print(filename)\n",
    "            try:\n",
    "                result = create_file(f\"{path}{filename}\")\n",
    "                run = re.findall(r'Run\\d',filename)[0]\n",
    "                if 'tp1' in filename.lower():\n",
    "                    generate_timing_conditions_FSL(result,f\"{subject_code}\\ses-1\\\\{subject_code}{run}\")\n",
    "                elif 'tp5' in filename.lower():\n",
    "                    generate_timing_conditions_FSL(result,f\"{subject_code}\\ses-2\\\\{subject_code}{run}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Problem with {filename}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68f24b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='LOG/'\n",
    "output_path = 'output/'\n",
    "for raw_file in os.listdir('LOG/'):\n",
    "    #check whether file contains subject's code\n",
    "    if len(re.findall(r'b\\d+',raw_file.lower()))>0:\n",
    "        #if it contains code, try create directory with subject code,\n",
    "        #if this directory exists, pass this action.\n",
    "        subject_code = re.findall(r'b\\d+',raw_file.lower())[0].upper()\n",
    "        try:\n",
    "            os.mkdir(f\"{output_path}sub-{subject_code}\")\n",
    "            os.mkdir(f'{output_path}sub-{subject_code}\\ses-1')\n",
    "            os.mkdir(f'{output_path}sub-{subject_code}\\ses-2')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        #\n",
    "        files = find_subject(subject_code,path)\n",
    "        for filename in files:\n",
    "            print(filename)\n",
    "            try:\n",
    "                result = create_file(f\"{path}{filename}\")\n",
    "                run = re.findall(r'Run\\d',filename)[0]\n",
    "                if 'tp1' in filename.lower():\n",
    "                    generate_timing_conditions_SPM(result,f\"{output_path}sub-{subject_code}\\ses-1\\\\sub-{subject_code}_ses-1_{run}\")\n",
    "                elif 'tp5' in filename.lower():\n",
    "                    generate_timing_conditions_SPM(result,f\"{output_path}sub-{subject_code}\\ses-2\\\\sub-{subject_code}_ses-2_{run}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Problem with {filename}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfc169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
